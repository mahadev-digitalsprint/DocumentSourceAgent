# ğŸ“Š FinWatch â€” Financial Document Intelligence & Website Monitoring

> Automatically discovers, downloads, classifies, and extracts metadata from financial PDF documents across company investor-relations websites. Monitors pages for changes and sends daily email digests.

---

## ğŸ— Architecture

```
finwatch/
â”œâ”€â”€ backend/                  # FastAPI + LangGraph agents
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ agents/           # 8 pipeline agents (crawl â†’ email)
â”‚   â”‚   â”œâ”€â”€ api/              # REST endpoints (companies, documents, jobs, â€¦)
â”‚   â”‚   â”œâ”€â”€ workflow/         # LangGraph DAG wiring
â”‚   â”‚   â”œâ”€â”€ models.py         # SQLAlchemy ORM
â”‚   â”‚   â”œâ”€â”€ database.py       # PostgreSQL engine
â”‚   â”‚   â”œâ”€â”€ config.py         # Settings (reads from .env)
â”‚   â”‚   â”œâ”€â”€ tasks.py          # Celery tasks
â”‚   â”‚   â””â”€â”€ main.py           # FastAPI app
â”‚   â””â”€â”€ .env                  # â† secrets go here (never commit)
â””â”€â”€ frontend/                 # Streamlit multi-page app
    â”œâ”€â”€ Home.py               # Dashboard landing page (combined)
    â”œâ”€â”€ api_client.py         # Shared HTTP client â†’ port 8080
    â””â”€â”€ pages/
        â”œâ”€â”€ 2_Companies.py    # Add / manage companies
        â”œâ”€â”€ 3_WebWatch.py     # Page-change monitor
        â”œâ”€â”€ 4_Documents.py    # Financial & non-financial docs
        â”œâ”€â”€ 5_Metadata.py     # LLM-extracted metadata
        â”œâ”€â”€ 6_Changes.py      # 24h change log
        â”œâ”€â”€ 7_Email_Alerts.py # Email digest config
        â”œâ”€â”€ 8_Settings.py     # System settings
        â””â”€â”€ 9_Analytics.py    # Charts & insights
```

---

## âš™ï¸ Prerequisites

| Requirement | Version | Notes |
|------------|---------|-------|
| Python | 3.11+ | |
| PostgreSQL | 14+ | Azure PostgreSQL or local |
| Redis | 7+ | Only needed for scheduled/async jobs |
| Azure OpenAI **or** OpenAI | â€” | LLM extraction |
| Tavily API key | â€” | PDF discovery |
| Firecrawl API key | â€” | Deep crawl (optional, gracefully skipped if no credits) |

---

## ğŸš€ Quick Start (Local â€” No Docker)

### 1 Â· Clone & enter

```bash
git clone https://github.com/mahadev-digitalsprint/DocumentSourceAgent.git
cd DocumentSourceAgent/finwatch
```

### 2 Â· Create & activate virtual environment

```bash
python -m venv .venv

# Windows
.venv\Scripts\activate

# macOS / Linux
source .venv/bin/activate
```

### 3 Â· Install dependencies

```bash
pip install -r backend/requirements.txt
pip install streamlit pandas openpyxl plotly requests
```

### 4 Â· Configure environment variables

```bash
cp backend/.env.example backend/.env
```

Open `backend/.env` and fill in:

```env
# â”€â”€ Database â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DATABASE_URL=postgresql://user:password@host:5432/finwatch

# â”€â”€ LLM (use one of the two) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_KEY=your-azure-openai-key
AZURE_OPENAI_DEPLOYMENT=gpt-4o

# OR plain OpenAI:
OPENAI_API_KEY=sk-...

# â”€â”€ Crawling â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TAVILY_API_KEY=tvly-...
FIRECRAWL_API_KEY=fc-...          # optional

# â”€â”€ Email (Office365) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SMTP_HOST=smtp.office365.com
SMTP_PORT=587
SMTP_USER=your@email.com
SMTP_PASSWORD=your-password
EMAIL_FROM=your@email.com
```

### 5 Â· Start the backend

```bash
cd backend
python -m uvicorn app.main:app --host 127.0.0.1 --port 8080 --reload
```

âœ… API docs live at: **http://127.0.0.1:8080/docs**

### 6 Â· Start the frontend (new terminal)

```bash
cd frontend
python -m streamlit run Home.py --server.port 8501
```

âœ… Dashboard live at: **http://localhost:8501**

---

## ğŸ³ Docker Compose (Recommended for production)

```bash
cd finwatch
docker-compose up --build
```

| Service | URL |
|---------|-----|
| FastAPI backend | http://localhost:8080 |
| Streamlit frontend | http://localhost:8501 |
| Celery worker | (background) |
| Redis | localhost:6379 |

---

## ğŸ”„ Pipeline Agents

The pipeline runs as a **LangGraph DAG** with 8 nodes:

```
M1 Crawl â†’ M2 Download â†’ M3 OCR â†’ M4 Classify â†’ M5 WebWatch â†’ M6 Extract â†’ M7 Excel â†’ M8 Email
```

| Agent | What it does |
|-------|-------------|
| **M1 â€” Crawl** | Discovers PDF URLs via 5 strategies: Firecrawl, Tavily, SEC EDGAR, BeautifulSoup, Regex |
| **M2 â€” Download** | Downloads each PDF, checks for duplicates via SHA-256 hash |
| **M3 â€” OCR** | Extracts text; runs Tesseract OCR on scanned/image PDFs |
| **M4 â€” Classify** | Assigns `FINANCIAL\|TYPE` or `NON_FINANCIAL\|TYPE` from 18 document types |
| **M5 â€” WebWatch** | Snapshots IR pages, detects added/deleted/changed pages |
| **M6 â€” Extract** | LLM extracts 15-field financial or 13-field non-financial metadata |
| **M7 â€” Excel** | Generates 7-sheet styled Excel workbook |
| **M8 â€” Email** | Sends HTML digest email via Office365 SMTP |

### Running the pipeline

**From the Dashboard UI** â†’ click **â–¶ Start Pipeline** (requires Redis + Celery)

**Via API (no Celery needed)**:
```bash
# Run for one company
curl -X POST http://localhost:8080/api/jobs/run-direct/1

# Run for all active companies
curl -X POST http://localhost:8080/api/jobs/run-all-direct
```

**Via Celery (production)**:
```bash
# Start worker
celery -A app.celery_app worker --loglevel=info

# Start beat scheduler (hourly WebWatch + daily digest)
celery -A app.celery_app beat --loglevel=info
```

---

## ğŸ“„ API Reference

Full interactive docs: **http://localhost:8080/docs**

| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/api/companies/` | List all companies |
| POST | `/api/companies/` | Add single company |
| POST | `/api/companies/bulk` | Add multiple companies |
| DELETE | `/api/companies/{id}` | Delete company |
| PATCH | `/api/companies/{id}/toggle` | Toggle active/inactive |
| GET | `/api/documents/` | List documents (filters: company_id, doc_type, status) |
| GET | `/api/documents/metadata/` | List all extracted metadata |
| GET | `/api/documents/changes/` | Document change log |
| GET | `/api/webwatch/snapshots` | Page snapshots |
| GET | `/api/webwatch/changes` | WebWatch page changes |
| POST | `/api/jobs/run-direct/{company_id}` | Run pipeline (no Celery) |
| POST | `/api/jobs/run-all-direct` | Run all companies (no Celery) |
| POST | `/api/jobs/run/{company_id}` | Queue via Celery |
| POST | `/api/jobs/run-all` | Queue all via Celery |

---

## ğŸ“‹ Adding Companies

**Single**: Companies page â†’ tab "Single Company" â†’ fill name + URL â†’ Add

**Multiple at once**: Companies page â†’ tab "Multiple Companies" â†’ fill the form rows â†’ Add All

**CSV Upload**: Companies page â†’ tab "Bulk CSV Upload"

CSV format:
```csv
company_name,website_url,crawl_depth
ICICI Bank,https://www.icicibank.com/investor-relations,3
Infosys,https://www.infosys.com/investors,3
TCS,https://www.tcs.com/investors,3
```

> âš ï¸ Always enter **Company Name** as a readable name (e.g. `ICICI Bank`), not a domain (`www.icici.bank.in`). The crawler auto-cleans domain-style names but proper names give better search results.

---

## ğŸ—‚ Document Classification

Documents are classified into **18 types** in two categories:

**Financial** (10 types): `ANNUAL_REPORT Â· QUARTERLY_RESULTS Â· HALF_YEAR_RESULTS Â· EARNINGS_RELEASE Â· INVESTOR_PRESENTATION Â· FINANCIAL_STATEMENT Â· IPO_PROSPECTUS Â· RIGHTS_ISSUE Â· DIVIDEND_NOTICE Â· CONCALL_TRANSCRIPT`

**Non-Financial** (8 types): `ESG_REPORT Â· CORPORATE_GOVERNANCE Â· PRESS_RELEASE Â· REGULATORY_FILING Â· LEGAL_DOCUMENT Â· HR_PEOPLE Â· PRODUCT_BROCHURE Â· OTHER`

Stored as `CATEGORY|TYPE`, e.g. `FINANCIAL|ANNUAL_REPORT`.

---

## ğŸ›  Troubleshooting

| Symptom | Fix |
|---------|-----|
| `Cannot connect to backend (port 8080)` | Run `uvicorn` in the `backend/` directory |
| `Celery not available: 503` | Use `/jobs/run-direct` endpoints; or start Redis + Celery worker |
| `[EDGAR] Could not resolve CIK` | EDGAR only indexes US-registered companies; non-US companies use Tavily + BS4 strategies |
| `[FIRECRAWL] Skipped (insufficient credits)` | Normal â€” other 4 strategies still run |
| PDF count = 0 | Check the company URL points to an investor-relations page with PDFs |
| DB tables missing | Backend auto-creates tables on startup via `models.Base.metadata.create_all` |

---

## ğŸ” Security

- All secrets in `backend/.env` â€” **never committed** (in `.gitignore`)
- `.env.example` provided as a template
- CORS allows all origins (restrict in production)

---

## ğŸ“¦ Tech Stack

`FastAPI` Â· `LangGraph` Â· `SQLAlchemy` Â· `PostgreSQL` Â· `Celery` Â· `Redis` Â· `Azure OpenAI` Â· `Streamlit` Â· `Pandas` Â· `openpyxl` Â· `Firecrawl` Â· `Tavily` Â· `BeautifulSoup4` Â· `pdfminer` Â· `Tesseract OCR`